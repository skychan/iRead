# Ch4 朴素贝叶斯法

对于给定的训练数据集，首先基于特征条件独立假设学习输入/输入的联合概率分布，然后基于此模型，对给定的输入 $x$，利用贝叶斯定理求出后验概率最大的输出 $y$

*与贝叶斯估计是不同的概念*

实现简单，学习与预测效率都很高，是一种常用的方法。


<!-- TOC -->

- [1. 朴素贝叶斯的学习与分类](#1-朴素贝叶斯的学习与分类)
- [2. 朴素贝叶斯法的参数估计](#2-朴素贝叶斯法的参数估计)

<!-- /TOC -->

## 1. 朴素贝叶斯的学习与分类

$P(X,Y)$ 是输入空间上的随机向量 $X$ 与输出空间上的随机变量 $Y$ 的联合概率分布。

训练数据集：

$$
T = \{(x_1,y_1),(x_2,y_2),\dots, (x_N,y_N)\}
$$

朴素贝叶斯法通过训练数据集学习联合概率分布 $P(X,Y)$。具体来说，学习先验概率分布：

$$
P(Y=c_k) \quad k = 1,2,\dots,K
$$

及条件概率分布：

$$
P(X=x|Y=c_k) = P(X^{(1)} = x^{(1)}, X^{(2)} = x^{(2)},\dots,X^{(N)} = x^{(N)}|Y=c_k)
$$

条件概率分布具有指数级的参数。

朴素贝叶斯法对条件概率分布作了 **条件独立性** 的假设：用于分类的特征在类别确定的条件下都是条件独立的。

$$
P(X=x|Y=c_k) = \prod_{j=1}^{n} P(X^{(j)}=x^{(c)}|Y=c_k)
$$

朴素贝叶斯法学属于生成模型。

分类：对给定的输入 $x$，通过学习到的模型计算后验概率分布 $P(Y=c_k|X=x)$，将后验概率最大的类作为 $x$ 的类输出。

$$
\begin{aligned}
P(Y=c_k|X=x) & = \frac {P(X=x|Y=c_k)P(Y=c_k)} {\sum_k P(X=x|Y=c_k)P(Y=c_k)}\\
& = \frac {P(Y=c_k)\prod_jP(X=x|Y=c_k)} {\sum_k P(Y=c_k)\prod_jP(X=x|Y=c_k)}
\end{aligned}
$$

朴素贝叶斯分类器可以表示为：

$$
y = f(x) = \argmax_{c_k} \frac {P(Y=c_k)\prod_jP(X=x|Y=c_k)} {\sum_k P(Y=c_k)\prod_jP(X=x|Y=c_k)}
$$

由于上式分母中，不同的 $c_k$ 对结果不产生影响，因此等价于:

$$
y = f(x) = \argmax_{c_k} P(Y=c_k)\prod_jP(X=x|Y=c_k)
$$

后验最大化准则：

$$
f(x) = \argmax_{c_k} P(c_k|X=x)
$$

## 2. 朴素贝叶斯法的参数估计

1. 先验的极大似然估计是

$$
P(Y=c_k) = \frac{\sum_{i=1}^N \mathbb 1_{y_i=c_k}}{N}
$$

条件概率的极大似然估计是

$$
P(X^{(j)}=a_{jl}|Y=c_k) = \frac{\sum_{i=1}^N \mathbb 1_{x_i^{(j)}=a_{jl},y_i=c_k}}{\sum_{i=1}^N \mathbb 1_{y_i=c_k}}
$$


2. 贝叶斯估计

用极大似然估计可能会出现所要估计的概率值为0的情况，会影响到后续的计算结果，使分类偏差。

$$
P_\lambda(X^{(j)}=a_{jl}|Y=c_k) = \frac{\sum_{i=1}^N \mathbb 1_{x^{(j)}, y_j=c_k} + \lambda}{\sum_{i=1}^N \mathbb 1_{y_i=c_k} + S_j\lambda}
$$

$\lambda=0$ 是极大似然估计，$\lambda=1$是拉普拉斯平滑。

先验的贝叶斯估计：

$$
P_\lambda(Y=c_k) = \frac{\sum_{i=1}^N \mathbb 1_{y_i=c_k}  + \lambda}{N + k\lambda}
$$

条件概率同极大似然估计一致。