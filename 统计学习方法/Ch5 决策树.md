# Ch5 决策树

决策树是一种基本的分类与回归方法。

可以是 if-then 的规则集合，也可以是定义在特征空间与类空间上的条件概率分布。

优点：

- 模型具有可读性
- 分类快速

1. **学习时**：利用训练数据，根据损失函数最小化的原则建立决策树模型。
2. **预测时**：对新的数据，利用决策树模型进行分类。

步骤：
1. 特征选择
2. 决策树生成
3. 决策树的修剪

## 决策树模型与学习

决策树可以表示给定特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分上。特征空间被划分成互不相交的单元或区域，并在每个单元定义一个类的概率。决策树的一条路径对应于划分中的一个单元。

决策树所表示的条件概率分布由各个单元给定的条件下累得条件概率分布组成。

$X$ 表示特征的随机变量，$Y$ 表示类的随机变量。

决策树学习本质上是从训练数据集中归纳出一组分类规则，与训练数据集不相矛盾的决策树可能有多个，也可能一个都没有。需要训练一个与训练数据矛盾较小的决策树，同时具有很好的*泛化能力*。

损失函数：正则化的极大似然函数。

从所有可能的决策树中选取最优决策树是 NP 完全问题，所以在现实中决策树学习算法通常采用启发式的方法，近似求解这一问题优化问题。

决策树算法通常是一个递归的选择最优化特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类过程。

生成树之后，为了避免过拟合，通常需要自下而上地剪枝，将树变得简单，从而提高泛化能力。就是去掉过于细分的叶节点，使其回退到父节点甚至更高的节点。

决策树的生成只考虑局部最优，而剪枝则考虑全局最优。

## 特征选择

选取对训练数据具有分类能力的特征，这样可以提高决策树学习效率。若利用一个特征进行分类的结果与随机分类的结果没有很大差别，则这个特征是没有分类能力的。

选择的准则：**信息增益**或**信息增益比**

### 信息增益

1. 随机变量 $X$ 的熵：

$$
H(X) = -\sum_{i=1}^n p_i \log p_i
$$

- 2 为底，则单位为 bit
- e 为底，则单位为 nat

通常记为 $H(p)$

2. 条件熵：

$$
H(Y|X) = \sum_{i=1}^n p_iH(Y|X=x_i)
$$

3. 信息增益

表示得知特征 $X$ 的信息而使得 $Y$ 的信息不确定减少的程度。

特征 $A$ 给训练数据 $D$ 的信息增益：

$$
g(D, A) = H(D) - H(D|A)
$$

右边一般称为*互信息*

4. 信息增益比

信息增益的大小是相对于训练数据集而言的，并没有绝对的意义，在分类问题困难时，也就是在训练数据集的经验熵大的时候，信息增益值会偏大。使用信息增益比可以对这一问题进行校正。

$$
g_R(D, A) = \frac{g(D,A)}{H(D)}
$$

## 决策树的生成

### ID3 算法

核心：在决策树各个节点熵应用信息增益准则进行特征选择，递归地构建决策树。相当于用一个极大似然法进行概率模型的选择。

缺点：只有树的生成，产生的树容易过拟合。

### C4.5算法

与 ID3 相似，在生成过程中，使用信息增益比来选择特征。

## 决策树的剪枝

剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。

设树 $T$ 的叶节点个数为 $|T|$，$t$ 是其中一个叶节点，有 $N_t$ 个样本，其中 $k$类的有 $N_{tk}$ 个，那么损失函数定义为：

$$
C_\alpha(T) = \sum_{t=1}^{|T|} N_tH_t(T) + \alpha |T|
$$

其中，经验熵

$$
H_t(T) = -\sum_{k}\frac{N_{tk}}{N_t} \log \frac{N_{tk}}{N_t}
$$

若记 

$$
C(T) = \sum_{t=1}^{|T|} N_tH_t(T)
$$

表示模型对训练数据的误差，$|T|$ 是模型的复杂度。$\alpha$ 参数控制两者之间的选择，越大促使使用简单的模型，越小促使使用复杂的模型。

剪枝考虑修剪后，模型的损失函数是否变小，若变小，则进行剪枝。可以使用动态规划来实现。

## CART 算法

分类与回归树（Classification and Regression Tree）即可用于分类，也可用于回归。 包含了生成和剪枝的过程。