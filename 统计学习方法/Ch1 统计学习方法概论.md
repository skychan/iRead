# Ch1 统计学习方法概论

<!-- TOC -->

- [1. 统计学习](#1-统计学习)
- [2. 监督学习](#2-监督学习)
- [3. 统计学三要素](#3-统计学三要素)
    - [3.1. 模型](#31-模型)
    - [3.2. 策略](#32-策略)
    - [3.3. 算法](#33-算法)
- [4. 模型评估与模型选择](#4-模型评估与模型选择)
- [5. 正则化与交叉验证](#5-正则化与交叉验证)
- [6. 泛化能力](#6-泛化能力)
- [7. 生成模型和判别模型](#7-生成模型和判别模型)
    - [7.1. 生成方法](#71-生成方法)
    - [7.2. 判别方法](#72-判别方法)
- [8. 分类问题](#8-分类问题)
- [9. 标注问题](#9-标注问题)
- [10. 回归问题](#10-回归问题)

<!-- /TOC -->

## 1. 统计学习

统计学习（Statistical Learning） 是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。

- 以计算机网络为平台
- 数据驱动科学
- 对数据进行云测和分析
- 以方法为中心，构建模型
- 交叉学科：概率论、统计学、信息论、计算理论、最优化理论、计算机科学

**主要方法：**

1. *监督学习*：从给定的有限的用于学习的训练数据集合（training data）出发，假设数据是独立同分布产生的，并假设学习的模型属于某个函数的集合（假设空间），应用某个评价准则（evaluation criterion），从假设空间选取一个最优的预测。
2. 非监督学习
3. 半监督学习
4. 强化学习

**三要素：**

- 模型
- 策略
- 算法

## 2. 监督学习

变量用大写字母，变量所取的值用小写字母。

一般一个输入是一个实例，所有的实例存在的空间称为特征空间。输入实例:

$$
x_i = \left[x_i^{(1)}, x_i^{(2)}, x_i^{(3)},\dots,x_i^{(n)}\right]^T
$$

通常，训练集表表示为：

$$
T = \left\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N) \right\}
$$

输入输出对又称为样本。

- 回归问题：输入变量与输出变量均为连续变量的预测问题
- 分类问题：输出变量为有限个离散变量的预测问题
- 标注问题：输入与输出变量序列的预测问题

**基本假设**

X,Y 遵循联合概率分布 $P(X,Y)$。

## 3. 统计学三要素

### 3.1. 模型

$$
\mathcal{F} = \left\{ f | Y_\theta(X),\theta\in\bm R^n \right \}
$$

$X$ 和 $Y$ 通常是定义在 $\mathcal{X}$ 和 $\mathcal{Y}$ 空间上的变量，$\mathcal{F}$ 通常是由一个参数向量决定的函数族。 $\theta$ 是参数空间。

或者使用条件概率分布族亦可：

$$
\mathcal{F} = \left\{ P | P_\theta(Y|X), \theta\in\bm R^n\right\}
$$

### 3.2. 策略

- 损失函数：度量模型一次预测的好坏
- 风险函数：度量平均意义下模型预测的好坏（损失值期望，风险最小的模型）

平均损失：经验风险

> 经验风险最小化

$$
\min_{f\in\mathcal{F}} \frac 1 N \sum_{i=1}^N L\left(y_i,f(x_i)\right)
$$

容易过拟合。

> 结构风险最小化

$$
\min_{f\in\mathcal{F}} \frac 1 N \sum_{i=1}^N L\left(y_i,f(x_i)\right) + \lambda J(f)
$$

$J(f)$ 为模型的复杂度。

### 3.3. 算法

可以使用已有的，也可以自己开发。

## 4. 模型评估与模型选择

- 训练误差：在训练集上的平均损失
- 测试误差：在测试集上的平均损失

通常，测试误差小的模型更有效，对未知数据的预测能力越强，也称为泛化能力。

## 5. 正则化与交叉验证

是模型选择的典型方法，使用结构风险最小化策略进行实现。

一般正则化项是一个单调递增的函数，模型越复杂，正则化值越大。

正则化项的选择符合奥卡姆剃刀原理。从贝叶斯估计的角度来看，正则化项对应于模型的先验概率。可以假设复杂的模型有较大的先验概率，简单的模型有较小的先验概率。

## 6. 泛化能力

对未知数据的预测误差即为泛化误差。

学习方法的泛化能力分析往往是通过研究泛化误差的概率上界进行的，简称泛化误差上界。

**泛化误差上界：** 
- 是样本容量的函数，容量增加，上界趋于0
- 是假设空间容量的函数，假设空间越大，越难学，上界越大

$$
R(f) \le \hat R(f) + \sqrt{\frac 1 {2N} \left(\log d + \log \frac 1 \delta \right)}
$$

$d$ 假设空间函数个数

## 7. 生成模型和判别模型

监督学习方法可以分为生成方法和判别方法，对应生成模型和判别模型。

### 7.1. 生成方法

由数据学习联合概率分布 $P(Y,X)$，然后求出条件概率分布的预测模型：

$$
P(Y|X) = \frac {P(X,Y)}{P(X)}
$$

模型给定了输入 $X$ 产生输出 $Y$ 的生成关系。

典型的生成模型：
- 朴素贝叶斯
- 隐马尔科夫模型

特点：
- 可以还原联合概率分布 $P(X,Y)$
- 收敛快
- 存在隐变量仍然可以用

### 7.2. 判别方法

由数据直接学习决策函数 $f(X)$ 或者条件概率分布 $P(Y|X)$ 作为预测模型

> 判别方法关心对给定的输入 $X$ 应该预测怎样的输出 $Y$

典型的判别模型：
- $k$ 临近
- 感知机
- 决策树
- 逻辑斯谛回归
- 最大熵
- 支持向量机
- 提升方法
- 条件随机场

特点：
- 直接学习，直面预测，准确率更高
- 可以对数据进行各种程度上的抽象、定义特征并使用特征，简化学习

## 8. 分类问题

分类问题包括学习和分类两个过程

指标一般是分类准确率：给定的测试集，分类器正确分类的样本与总样本之比。

二分类还常用精确率和召回率进行评价。

## 9. 标注问题

tagging

是分类问题的一个推广，是结构预测的简单形式。

- 输入：一个观测序列
- 输出：一个标记序列或者状态序列
- 目的：学习一个模型，使得能够对观测序列给出标记序列作为预测


分为学习和标注两个过程

常用方法：
- 隐马尔科夫模型
- 条件随机场

场景：
- 信息抽取
- 自然语言处理

## 10. 回归问题

用于预测输入变量和输出变量之间的关系。回归模型就是表示从输入到输出的映射函数，回归问题的学习等价于拟合函数
