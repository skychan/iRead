# Ch3 $k$ 近邻法

即 KNN，是一种基本分类与回归方法。本章讨论分类中的 KNN。

- 输入：实例的特征向量
- 输出：实例的类别

对新的实例，根据 $k$ 个最临近。$k$ 值的选择、距离度量及分类决策规则，是 KNN 的三要素。

<!-- TOC -->

- [1. $k$ 近邻算法](#1-k-近邻算法)
- [$k$ 近邻模型](#k-近邻模型)
- [实现：$kd$ 树](#实现kd-树)

<!-- /TOC -->

## 1. $k$ 近邻算法

离 $x$ 最近，$\mathcal N_k(x)$ 表示涵盖 $k$ 个点的邻域。

多数表决规则

$$
y = \argmax_{c_j} \sum_{x_i\in\mathcal N_k(x)} \bm 1_{y_i=c_j}
$$

## $k$ 近邻模型

1. $L_p$ 距离：

$$
L_p(x_i, x_j) = \left(\sum_{l=1}^n\left|x_i^{(l)} - x_j^{(l)}\right|^p\right)^{\frac 1 p}
$$

2. $k$ 值选择

值越小，越容易过拟合。
值越大，模型就变得越简单，预测不起作用。

一般采用交叉验证法进行确定。

3. 分类规则

往往采用多数表决，即由输入实例的 $k$ 个近邻的训练实例中多数决策器决定。

## 实现：$kd$ 树

主要考虑如何的对训练数据进行快速 $k$ 近邻搜索。这点在特征空间的维数大及训练数据容量大时尤为重要。

**kd 树**：一种对 $k$ 维空间中的实例点进行存储，以便对其进行快速检索的树形数据结构。是一种二叉树。构造 kd 树相当于不断地用垂直于坐标轴的超平面将 $k$ 维空间切分，构成一系列 $k$ 维超矩形区域。每个节点对应一个超矩形区域。

