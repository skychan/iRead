# Ch9 EM 算法及其推广

EM 算法是一种迭代算法，用于含有隐含变量的概率模型参数的极大似然估计，或极大后验概率估计。

EM 算法的每次迭代都由两步组成：
1. E 步，求期望
2. M 步，求极大值

所以也称为期望极大算法

<!-- TOC -->

- [1. EM 算法的引入](#1-em-算法的引入)
    - [1.1. EM 算法](#11-em-算法)

<!-- /TOC -->

## 1. EM 算法的引入

概率模型有时既含有观测变量，又含有隐变量或潜在变量。

如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计，或用贝叶斯估计模型参数。但是当模型含有隐含时，就不能简单地使用这些估计方法。

### 1.1. EM 算法

将观测数据表示为 $Y=(Y_1, Y_2,\dots,Y_n)^T$，未观测数据表示为 $Z=(Z_1, Z_2,\dots,Z_n)^T$，则观测数据的似然函数为：

$$
P(Y|\theta) = \sum_Z P(Y,Z|\theta) = \sum_Z P(Z|\theta)P(Y|Z, \theta)
$$

考虑模型参数 $\theta$ 的极大似然估计，即

$$
\hat \theta = \argmax_\theta \log P(Y|\theta)
$$

这个问题没有解析解，只有通过迭代的方法求解。EM 算法就是可以用于求解这个问题的一种迭代算法。

EM 算法和初值的选择有关，选择不同的初值可能得到不同的参数估计值。

- Y: 观测随机变量的数据，也称不完全数据
- Z: 隐随机变量的数据

Y,Z 连在一起称为**完全数据**

**算法**
> 输入：观测变量数据 $Y$, 隐变量是数据 $Z$，联合分布 $P(Y,Z|\theta)$，条件分布 $P(Z|Y,\theta)$
> 输出：模型参数 $\theta$

1. 选择参数的初值 $\theta^{(0)}$，开始迭代；
2. E 步：记 $\theta^{(i)}$ 为第 $i$ 次迭代参数 $\theta$ 的估计值，在第 $i+1$ 次迭代的 E 步，计算

$$
\begin{aligned}
Q(\theta,\theta^{(i)}) &= E_Z[\log P(Y,Z|\theta)|Y,\theta^{(i)}] \\
& = \sum_Z\log P(Y,Z|\theta) P(Z|Y,\theta)
\end{aligned}
$$

3. M 步：求使 $Q(\theta,\theta^{(i)})$ 极大化的 $\theta$，确定第 $i+1$ 次迭代的参数估计值 $\theta^{(i+1)}$

$$
\theta^{(i+1)} = \argmax_\theta Q(\theta, \theta^{(i)})
$$

4. 重复 2，3 步，直到收敛

EM 算法的核心 $Q(\theta,\theta^{(i)})$ 函数。
