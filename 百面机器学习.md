# iRead《百面机器学习》
<!-- TOC -->

- [1. 特征工程](#1-特征工程)
    - [1.1. 特征归一化/正则化（Normalization）](#11-特征归一化正则化normalization)
    - [1.2. 类别型特征（Categorical Feature）](#12-类别型特征categorical-feature)
    - [1.3. 高维组合特征的处理](#13-高维组合特征的处理)
    - [1.4. 组合特征](#14-组合特征)
    - [1.5. 文本表示模型](#15-文本表示模型)
    - [1.6. Word2Vec](#16-word2vec)
    - [1.7. 图像数据不足时的处理方法](#17-图像数据不足时的处理方法)

<!-- /TOC -->
## 1. 特征工程

在机器学习中，数据和特征是“米”，模型和算法是“巧妇”。

特征工程，是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。

实际工作中，特征工程旨在去除原始数据中的杂质和冗余，设计更高的特征以刻画求解的问题和预测模型之间的关系。

### 1.1. 特征归一化/正则化（Normalization）

消除数据特征之间的量纲影响，使得不同指标之间具有可比性。使各指标处于同一个数值量级，以便进行分析。

> 为什么要对数值类型的特征做归一化？常见的归一化有哪些方式？

对数值类型的特征做归一化可以将所有的特征统一到一个大致相同的数值区间内，常用的方法主要有以下两种：

1. 线性函数归一化（Min-Max Scaling）

对原始数据映射到 $[0,1]$

$$
X_{norm} = \frac{X-X_{min}}{X_{max}-X_{min}}
$$

2. 零值归一化（Z-Score Normalization）

将原始数据映射到均值为0、标准差为1的分布上。

$$
z = \frac{x-\mu}{\sigma}
$$

不同的特征如果取值范围不同，那么在学习速率相同的情况下，会出现更新速度不同的现象，需要迭代多次才能找到较优解。一般，需要通过梯度下降法求解的模型通常是需要归一化的，包括**线性回归、逻辑回归、支持向量机、神经网络**等模型。但是，对于**决策树**模型则并不适用。比如C4.5，决策树在进行节点分裂时，主要依据数据集关于特征信息的增益比，这和特征是否经过归一化无关。因为归一化不会改变样本在特征上的信息增益。

### 1.2. 类别型特征（Categorical Feature）
主要指性别（男、女），血型（A、B、AB、O）等只在有限选项内取值的特征。
原始输入通常是字符串形式，除了决策树等少数模型能直接处理字符串形式的输入，一般类别特征必须转换成数值型特征才能正确工作。

主要的方式包括：
- 序号编码（Ordinal Encoding）
- 独热变化（One-hot Encoding）
- 二进制编码（Binary Encoding）

> 在对数据进行预处理时，应该怎样处理类别型特征？
1. 序号编码

通常用于处理类别间具有大小关系的数据。按照大小关系对类别特征赋予一个数值ID，转换后依然保留大小关系。

2. 独热编码

通常用于处理类别间不由有大小关系的特征。需要注意的是：
- 使用稀疏向量来节省空间。
- 配合特征选择来降低维度。

高维空间下两点之间的距离很难得到有效地衡量，在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易过拟合。通常只有部分维度对分类、预测有帮助。

3. 二进制编码

利用二进制对ID 进行哈希映射，且维度少于独热编码。

4. 其他编码方式
- Helmert Constrast
- Sum Constrast
- Polynomial Constrast
- Backward Difference Constrast

### 1.3. 高维组合特征的处理

为了提高复杂关系的拟合能力，在特征工程中，经常会把一阶离散特征两两组合，构成高阶组合特征。

$$
Y = \sigma(\sum_i\sum_j w_{ij}<x_i, x_j>)
$$

$w_{ij}$ 的维度等于 $|x_i|\cdot|x_j|$，这对于高维参数几乎无法学习，这种情况下，一种方法是，将用户和物品分别用$k$维的低维向量表示，这样 $w_{ij}=x'_i \cdot x'_j$，等价于矩阵分解。

### 1.4. 组合特征

简单地两两组合，依然容易存在参数过多、过拟合等问题，并且并不是所有的特征组合都是有意义的。因此，需要一种有效的方法来帮助我们找到应该对哪些特征进行组合。

有效地找到特征组合，可以参考决策树，根据原始输入和标签构造决策树，每一条从根节点到叶节点的路径，都可以看作是一种组合。可以采用梯度提升决策树来构造。该方法的思想是，每次都在构建的决策树残差上构建下一棵决策树。

### 1.5. 文本表示模型

- 词袋模型（Bag of Words）TF-IDF（Term Frequency-Inverse Document Frequency）
- 主题模型（Topic Model）
- 词嵌入模型（Word Embedding）

**词袋模型**：将每篇文章看成一个袋子词，并忽略每个词出现的顺序。这个词袋向量的每一个维度代表一个单词，而该维度的权重反映了这个词在原文中的重要程度。常用TD-IDF 来计算权重

$$
TF-IDF(t,d) = TF(t,d) \times IDF(t)
$$

$TF(t,d)$ 表示单词$t$在文档$d$中出现的平率，$IDF(t)$ 是逆文档频率，用来衡量单词$t$对表达语义的重要性

$$
IDF(t) = log \frac{文章总数}{包含t的文章数+1}
$$

如果一个词在非常多的文章里面都出现，那么它可能是一个比较通用的词汇，对于区分某篇文章特殊语义的贡献较小，因此需要对权重进行惩罚。

**N-Gram模型**：可以连续出现的$n$个词组成的词组（$N-gram$），也作为一个单独的特征向量放到向量表示中去，构成这个模型。在实际应用中，一般会对单词进行词干抽取（Word Stemming）处理，即将不同词性的单词统一成为同一词干的形式。

**主题模型**：主题模型用于从文本库中发现有代表性的主题（得到每个主题上面词的分布特性），并且能够计算出每篇文章的主题分布。

**词嵌入与深度学习模型**：词嵌入是一类将词向量化的模型的统称，核心思想是将每个词都映射成低维空间（50~300）上的一个稠密向量。K 维空间的每一维也可以看作一个隐含的主题，只不过不像主题模型中的主题那样直观。

通常，如果仅仅把词嵌入矩阵作为原文本的表示特征输入到机器学习模型中，很难得到令人满意的结果。因此，还需要在此基础上加工出更高层的特征。深度学习的结构在文本处理中取得了很好的效果，主要是由于它们能够更好地对文本进行建模，抽取出更高一些的语义特征。另一方面，也减少了网络中待学习的参数，提高了训练的速度也降低了过拟合的风险。

### 1.6. Word2Vec
实际上是一种浅层的神经网络模型，一般有CBOW 和 Skip-gram 两种网络结构。

- CBOW：目标是根据上下文出现的词频来预测当前词的生成概率
- Skip-gram：根据当前词来预测上下文各词的生成概率

这两个模型都可由输入层（独热）、映射层（隐含层，CBOW 中还需要将各隐含单元求和。）和输出层组成。

输出的每一个维度和词汇表中的一个单词相对应。最后，对输出层向量应用Softmax激活函数，可以计算出每个单词的生成概率。

Input-Hidden ($N\times K$)

Hidden-Output （$K\times N$）

可以通过反向传播算法实现，每次迭代时，将权重沿梯度更优的方向进行一小步。但是，由于softmax 中存在归一化，推导出来的迭代公式需要对词汇表中的所有单词进行遍历，使得迭代的过程非常缓慢。由此，产生了 Hierarchical Softmax 和 Negative Sampling 两种改进方法。


- 与 LDA（隐狄利克雷模型）的区别和联系

LDA：利用文档中单词的共现关系来对单词按主题聚类，也可理解为对“文档-单词”矩阵进行分解，得到“文档-主题”和“主题-单词”的两个概率分布。

Word2Vec：对“上下文-单词”矩阵进行分解。其中上下文有周围的几个单词组成，由此得到的词向量表示等多地融入了上下文共现的特征。

也就是说，如果两个单词所对应的Word2Vec向量相似度较高，那么它们很可能经常出现在同样的上下文中。

主题模型可以通过一定的结构调整，基于“上下文-单词”矩阵进行主题推理，词嵌入方法也可以根据“文档-单词”矩阵学习出词的隐含向量表示。

主题模型和词嵌入两类方法的不同在于：**模型本身**
1. 主题模型是一种基于概率图模型的生成模型，其似然函数可以些成若干条件概率连乘的形式，其中包括需要推测的隐含变量（主题）；
2. 词嵌入模型一般表达为神经网络的形式，似然函数定义在网络的输出之上，需要通过学习网络的权重以得到单词的稠密向量表示。

### 1.7. 图像数据不足时的处理方法
（可能会过拟合）

*迁移学习、生成对抗网络、上采样技术、数据扩充*

一个模型所能提供的信息一般来源于：
1. 训练数据中蕴含的信息
2. 模型的形成过程中（构造、学习、推理），人提供的先验信息

训练数据不足，则说明从原始数据中获取的信息比较少。那就需要更多的先验信息。

- 用在模型上：结构、条件假设、添加约束
- 用在数据上：调整、变换、扩展等

手段：
1. 基于模型方法（降低过拟合风险）：
    - 简化模型（非线性转化为线性）
    - 添加约束项以缩小假设空间（L1/L2 正则项）
    - 集成学习
    - Dropout 超参数
2. 基于数据
    - 扩充数据（Data Augmentation），基于一些先验知识，在保持特定信息的前提下，对原始数据进行适当变换。
        - 随机旋转、平移、缩放、裁剪、填充、左右翻转等
        - 在图像中的像素添加噪声扰动，比如椒盐噪声、高斯白噪声
        - 颜色变换
        - 改变图像的亮度、清晰度、对比度、锐度等
3. 先对图像进行特征抽取，然后在图像的特征空间内进行变换，利用一些数据扩充或者上采样技术，例如SMOTE(Synthetic Minority Over-sampling Technique)算法。
4. 生成模型：GAN
5. 迁移模型，比如借用一个在大规模数据集上预训练号的通用模型，并在针对目标任务的小数据集上进行微调。