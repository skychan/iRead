# 强化学习精要

## 1. 信息论基础

- “熵”：可以形容为“惊喜度”，概率越小的事件惊喜程度越大
- KL 散度，描述两个概率之间的差异的一种方法

$$
KL(p||q) = \sum_x p(x) log\frac{p(x)}{q(x)}
$$

两个分布越接近，KL 三度越小，两个分布越远，KL 散度越大。

## 2. 机器学习基础

- 平方损失函数：最终输出的结果是回归问题的一个连续型变量，看重每一个输出结果，损失梯度和分类有关，会让错误的分类都变得更加平均
- 交叉熵损失函数：最终输出是分类问题的一个离散 One-Hot 向量，只看重正确分类，损失梯度只和正确分类的预测结果有关

## 3. 优化算法

### 3.1. 梯度下降法

函数的梯度方向表示了函数值增长速度最快的方向。

### 3.2. 动量算法
一个已经结束的更新不会立即消失，而是以一定的形式衰减，剩下的能量将在继续优化中发挥作用。

使用了动量后，历史的更新会以衰减的形式不断作用在这个方向上，那么沿着-y+y两个方向的动量就可以相互抵消，而-x方向则会一直加强。

更好地穿越一些平坦的优化区域

**Nesterov 算法** 计算梯度在动量更新后的优化点，而不像动量算法在当前的目标点。

### 3.3. 共轭梯度法

选择优化方向和步长上更加智能。强调每一步优化迭代的质量

### 3.4. 自然梯度法

将每一轮迭代中对参数的更新转变为对模型效果的更新

## 4. Tensorflow 一些用法

Variable 表示方法

```python
# 第一种方法
w1 = tf.Variable(tf.random_uniform([2,4], -1, 1), name='w1')
b1 = tf.Variable(tf.zeros(4, dtype=np.float32), name='b1')
# 第二种方法
w2 = tf.get_variable('w2', shape=[4,1], initializer=contrib.layers.xavier_initializer())
b2 = tf.get_variable('b2', shape=[1], initializer=tf.constant(0))
```

3个核心部分：定义模型、定义目标函数和定义优化方法。

### 4.1. 构建计算图

1. 创建变量的 scope

```python
with tf.name_scope('123'):
    with tf.name_scope('456'):
        with tf.variable_scope('789'):
            a = tf.Variable(1, name='a')
            print(a.name)
            b = tf.get_variable('b', 1)
            print(b.name)

# output
123/456/789/a:0
789/b:0
```

在同一个 scope 内，同样的名字的 name_scope 被声明第二次时，scope 的名字并不会直接被复用出现，而是通过改名的形式创建一个全新的 scope。系统会自动改名。

variable_scope 和 name_scope 是两个有点相互独立的命名体系，name_scope 只通过 Variable 创建的变量有效，而 variable_scpoe 还可以为 get_variable 这个复用的变量的方法服务。

- 两种 scope 形成的命名空间将在 Variable 创建的变量上产生影响
- variable_scope 创建的命名空间将对 get_variable 的变量名产生影响
- 创建 named_scope 时，遇到同名的 scope，系统会自动改名创建一个新的 scope
- 创建 named_scope 时，可以通过将名字设置为 None 抹掉前面设定的所有命名空间的名字

2. 运算操作
在编程的时候，最好为每一个 Op 起名字。

## 5. 辅助工具 Gym 与 Baselines

### Wrapper

Gym 中定义的类，可以在既有环境的基础上添加更多的功能。

在创建时，需要传入一个 env 对象，它是一个已经创建好的环境对象。（创建好或者env本身都可以）

1. 如果实现了以下划线开始的同名方法，就会调用这个函数
2. 否则，就会调用env中对应的函数。

### Baseline

- Gym 实现了环境相关的
- Baseline Agent 相关的功能

## 强化学习基本算法
- 策略迭代
- 价值迭代
- 泛化迭代

策略是一种映射，它将环境的状态值 $s_t$ 映射到一个行动集合的概率分布或概率密度函数上。

$$
\bm{a}_t^*=argmax_{\bm{a}_{t,i}}p(\bm{a}_{t,i}|{\bm{s}_0 ,\bm{a}_0, \dots, \bm{s}_t})
$$

## 广义策略迭代

- 策略迭代的中心是策略函数，它通过反复执行“策略评估+策略提升”两个步骤，使得策略变得越来越好；
- 价值迭代法的中心是值函数，它通过利用动态规划的方法迭代更新值函数，并最终求出策略函数。

两种方法都十分看中自己关心的部分，可以选择忽略另一部分，因此两个方法都比较极端。

**定义**：定义养成恩迭代算法族，其中的算法都是由策略迭代和价值迭代组合而成的。

组合的方法和形式的很多。也成为泛化迭代。

## Q-Learning 基础

|基于模型（Model-based Problem）|无模型（Model-free Problem）|
|:--|:--|
|知晓状态转移概率|不知晓状态转移概率|

新算法的思路：

1. 确定一个初始策略
2. 用这个策略进行游戏，得到一些游戏序列（Episode）$\{s_1,a_1,s_2,a_2,\dots,s_n,a_n\}$
3. 一旦游戏的轮数达到一定数目，就可以认为这些游戏序列代表了当前策略与环境交互的表现，就可以将这些序列聚合起来，得到状态对应的值函数。
4. 得到了值函数，就相当于完成了策略评估的过程，这样就可以继续按照策略迭代的方法，进行策略改进的操作，得到更新后的策略。

关键问题：

- 如何得到这些游戏序列
- 如何使用序列进行评估

### 蒙特卡洛法

当概率已知的时候，使用 Bellman 方程

$$
q_\pi(s_t,a_t) = \sum_{s_{t+1}\in S} p(s_{t+1}|s_t, a_t) \left(r_{t+1}+v_\pi(s_{t+1})\right)
$$

无模型中，状态转移概率未知，使用期望来替代

$$
q_\pi(s_t,a_t) = E_{s_{t+1}\sim p, \pi} \left[ \sum_{k=0}^{\infty} \gamma^k r_{t+k} \right]
$$

在获得大量样本序列后

$$
\{s_t,a_t,s_{t+1}^i,a_{t+1}^i,s_{t+2}^i,a_{t+2}^i,\dots\}_{i=1}^N
$$

那么，上述公式可以近似为：

$$
q(s,a) \approx \frac 1 N \sum_{i=1}^N\sum_{k=0}^\infty \gamma^k r_{t+k}
$$

算法流程：

1. 让Agent 与环境交互后得到序列
2. 通过序列计算每一时刻的价值
3. 将这些价值累积到值函数中进行更新
4. 根据跟新的值函数更新策略